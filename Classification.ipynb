{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Classification.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"qtRAFaU-RsZ-","executionInfo":{"status":"ok","timestamp":1603349611858,"user_tz":240,"elapsed":85311,"user":{"displayName":"shams basir","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg8NJWd8PZvQ1q4IT0m-PJSHWn2nLzMCOXym7Ydiw=s64","userId":"09640498298136079307"}},"outputId":"16883f6b-0098-4eca-be8f-b83a953a863e","colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["# mount drive, install kaggle, download the data, and unzip\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","!pip install -q kaggle\n","%mkdir /root/.kaggle\n","%cp /content/gdrive/My\\ Drive/CMU11785-HW2P2/kaggle.json  /root/.kaggle/\n","%cd /\n","!kaggle datasets download -d cmu11785/20fall-hw2p2\n","!unzip -q 20fall-hw2p2.zip -d data"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n","/\n","Downloading 20fall-hw2p2.zip to /\n"," 98% 1.32G/1.35G [00:18<00:00, 110MB/s]\n","100% 1.35G/1.35G [00:18<00:00, 78.3MB/s]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lBnd-4bElmIH","executionInfo":{"status":"ok","timestamp":1603356421296,"user_tz":240,"elapsed":323,"user":{"displayName":"shams basir","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg8NJWd8PZvQ1q4IT0m-PJSHWn2nLzMCOXym7Ydiw=s64","userId":"09640498298136079307"}},"outputId":"06e88a15-699b-4e34-a94f-e8ed3d618f64","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# change directory to hw2_p2\n","%cd /content/gdrive/My\\ Drive/CMU11785-HW2P2/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/CMU11785-HW2P2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ArT3mZm7sX5X"},"source":["# importing the packages\n","import os\n","import numpy as np\n","from PIL import Image\n","import time\n","import datetime\n","import torch.optim as optim\n","import torch\n","from torchvision import transforms, datasets\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.metrics import roc_auc_score"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NyNw9NEYxxs3"},"source":["#smallest ResNET Block\n","class baseBlock(torch.nn.Module):\n","  def __init__(self,in_channel,out_channel,stride=1,shortcut=None):\n","      super(baseBlock,self).__init__()\n","      self.conv1 = torch.nn.Conv2d(in_channel,out_channel,stride=stride,kernel_size=3,padding=1)\n","      self.bn1   = torch.nn.BatchNorm2d(out_channel)\n","      self.conv2 = torch.nn.Conv2d(out_channel,out_channel,stride=1,kernel_size=3,padding=1)\n","      self.bn2   = torch.nn.BatchNorm2d(out_channel)\n","      self.shortcut = shortcut\n","\n","  def forward(self,x):\n","      output = F.relu(self.bn1(self.conv1(x)))\n","      #print(output.shape)\n","      output = self.bn2(self.conv2(output))\n","      #print(output.shape)\n","      if self.shortcut is not None:\n","        output += self.shortcut(x)\n","      output = F.relu(output)\n","      #print(output.shape)\n","      return output\n","\n","\n","class ResNet(torch.nn.Module):\n","  def __init__(self,num_layers,classes=10,feats=512):\n","      super(ResNet,self).__init__()\n","      self.conv1 = nn.Conv2d(3,64,kernel_size=3,stride=1,padding=1)\n","      self.bn1   = nn.BatchNorm2d(64)\n","      self.input_planes = 64\n","      self.layer1 = self._layer(64,  num_layers[0],stride=2)\n","      self.layer2 = self._layer(128,  num_layers[1],stride=2)\n","      self.layer3 = self._layer(256,  num_layers[2],stride=2)\n","      self.layer4 = self._layer(feats,num_layers[3],stride=2)\n","      self.avgPool = nn.AdaptiveAvgPool2d((2))\n","      self.fc  =  torch.nn.Linear(feats*2*2,classes)\n","  \n","  def _layer(self,planes,num_layers,stride=1):\n","      netLayers =[]\n","      shortcut = None\n","      if stride !=1 or self.input_planes != planes:\n","        shortcut = torch.nn.Sequential(torch.nn.Conv2d(self.input_planes,planes,kernel_size=1,stride=stride),\n","                        torch.nn.BatchNorm2d(planes))\n","      \n","      netLayers.append(baseBlock(self.input_planes,planes,stride=stride,shortcut=shortcut))\n","      self.input_planes = planes\n","      for i in range(1,num_layers):\n","          netLayers.append(baseBlock(self.input_planes,planes))\n","          self.input_planes = planes\n","      return torch.nn.Sequential(*netLayers)\n","\n","  def forward(self,x):\n","      x = F.relu(self.bn1(self.conv1(x)))\n","      #print(\"Conv1  :\",x.shape)\n","      x = self.layer1(x)\n","      #print(\"L_1:\",x.shape)\n","      x = self.layer2(x)\n","      #print(\"L_2:\",x.shape)\n","      x = self.layer3(x)\n","      #print(\"L_3:\",x.shape)\n","      x = self.layer4(x)\n","      #print(\"L_4:\",x.shape)\n","      x = self.avgPool(x)\n","      #print(\"avg:\",x.shape)\n","      x = torch.flatten(x,1)\n","      #print(\"flattened:\",x.shape)\n","      out =self.fc(x)\n","      #print(\"labels: \",x.shape)\n","      return x,out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dbbjiEDWJpqc","executionInfo":{"status":"ok","timestamp":1603351141717,"user_tz":240,"elapsed":506,"user":{"displayName":"shams basir","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg8NJWd8PZvQ1q4IT0m-PJSHWn2nLzMCOXym7Ydiw=s64","userId":"09640498298136079307"}},"outputId":"dbd12eac-ae82-4983-bd74-538545457e39","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["x= torch.randn(5,3,64,64)\n","feats,out = ResNet([1,1,1,1],4000,512)(x)\n","print(out.shape)\n","print(feats.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["torch.Size([5, 4000])\n","torch.Size([5, 2048])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CzaAMUvfTBnx"},"source":["def init_weights(m):\n","  if type(m) == nn.Conv2d:    \n","    nn.init.kaiming_normal_(m.weight)\n","    if m.bias is not None:\n","      nn.init.zeros_(m.bias)\n","      \n","  if type(m) == nn.Linear:\n","    nn.init.xavier_normal_(m.weight)\n","\n","  if type(m) == nn.BatchNorm2d:\n","    nn.init.ones_(m.weight)\n","    nn.init.zeros_(m.bias)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BVJYtSy_y3iD"},"source":["def save_model(net,epoch):\n","  current_time = datetime.datetime.now().strftime(\"%m-%d\")\n","  # Specify a path\n","  newDir(\"saved_model\")\n","  PATH = os.getcwd()+f\"/saved_model/state_dict_{current_time}_epoch_{epoch}.pt\"\n","  print(\"----------- saving the model ....\\n\") \n","  torch.save(net.state_dict(), PATH)\n","\n","\n","\n","def save_output(vector,name):\n","  current_time = datetime.datetime.now().strftime(\"%Y.%m.%d-%H:%M:%S\")\n","  print(\" Saving outputs ...\")\n","  newDir('output')\n","  PATH  = os.getcwd()+\"/output/\"\n","  np.save(PATH+ f'{name}_{current_time}.npy',vector)\n","\n","\n","def visualize(soln_vect,name):\n","# from HW1_P1\n","  print(f\"Saving graphs for \\\"{name}\\\"\")\n","  newDir('output')\n","  current_time = datetime.datetime.now().strftime(\"%Y.%m.%d-%H:%M:%S\")\n","  PATH = os.getcwd() + \"/output/\"\n","  try :\n","    plt.plot(soln_vect)\n","    plt.ylabel(name)\n","    plt.savefig(PATH +f\"{name}_{current_time}.png\")\n","  except Exception as e:\n","    traceback.print_exc()\n","    print(\"Error: Problems generating plots. See if a .png was generated in output folder\\n\")\n","\n","\n","class newDir:\n","  def __init__(self,name):\n","    self.name = name\n","    if not os.path.exists(name):\n","        os.mkdir(name)\n","        print(f\"{self.name} directory created! \")\n","    else :\n","        print(f\"{self.name} directory exists! \")\n","\n","class rmDir:\n","  def __init__(self,name):\n","    self.name = name\n","    if os.path.exists(name):\n","      os.rmdir(name)\n","      print(f\"{self.name} directory is removed! \")\n","    else :\n","      print(f\"{self.name} directory does not exist! \")\n","\n","\n","def get_myDataset(data_dir,train=False,val=False,test=False):\n","  transform_kwargs = {\n","\t\t'train': transforms.Compose([transforms.RandomHorizontalFlip(),\n","                        transforms.ToTensor(),\n","                        transforms.Normalize((0.485,0.456,0.406),(0.229,0.224,0.225))]),\n","\n","\t\t'val'  : transforms.Compose([transforms.ToTensor(),\n","                        transforms.Normalize((0.485,0.456,0.406),(0.229,0.224,0.225))]),\n","\t\n","\t\t'test' : transforms.Compose([transforms.ToTensor(),\n","                         transforms.Normalize((0.485,0.456,0.406),(0.229,0.224,0.225))])\n","\t\t\t}\n","  if train:\n","    root \t  = os.path.join(data_dir,'train_data/')\n","    transform = transform_kwargs['train']\n","  if val :\n","    root      = os.path.join(data_dir,'val_data/')\n","\t\t#print(\"root: \",root)\n","    transform = transform_kwargs['val']\n","  if test:\n","    root \t  = os.path.join(data_dir,'test_data/')\n","    transform = transform_kwargs['test']\n","  data_set = datasets.ImageFolder(root=root, transform = transform)\n","  return data_set\n","\n","\n","def get_data_loader(data_set,**kwargs):\n","\treturn torch.utils.data.DataLoader(dataset=data_set,**kwargs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ogg4F_2YVD__"},"source":["\n","def train(model, train_loader, criterion, optimizer):\n","  model.train()\n","  running_loss = 0.0\n","  start_time = time.time()\n","  for batch_idx, (data, target) in enumerate(train_loader):   \n","    optimizer.zero_grad()   # .backward() accumulates gradients\n","    data = data.to(device)\n","    target = target.to(device) # all data & model on same device\n","\n","    _,labels = model(data)\n","    loss = criterion(labels, target)\n","    running_loss += loss.item()\n","\n","    loss.backward()\n","    optimizer.step()\n","    torch.cuda.empty_cache()\n","    del data\n","    del target \n","    if (batch_idx+1) % 500 == 0:\n","      print('Batch_index : {:5d}, Avg Loss in 500 mini batches: {:4.4f}'.format(batch_idx+1, running_loss/500))\n","      running_loss = 0.0\n","  return running_loss "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BldhcOqrVyY1"},"source":["def test(model, test_loader, criterion):\n","  with torch.no_grad():\n","    model.eval()\n","    running_loss = 0.0\n","    total_predictions = 0.0\n","    correct_predictions = 0.0\n","    start_time = time.time()\n","    for batch_idx, (data, target) in enumerate(test_loader):   \n","        data = data.to(device)\n","        target = target.to(device)\n","        _,outputs = model(data)\n","        outputs = F.softmax(outputs,dim=1)\n","        _,predicted = torch.max(outputs.data, 1)\n","        total_predictions += target.size(0)\n","        correct_predictions += (predicted == target).sum().item()\n","        loss = criterion(outputs, target).detach()\n","        running_loss += loss.item()\n","        torch.cuda.empty_cache()\n","        del data\n","        del target\n","\n","  end_time = time.time()\n","  running_loss /= len(test_loader)\n","  acc = (correct_predictions/total_predictions)*100.0\n","  print('Testing Loss: %2.4f'%(running_loss)) \n","  print('Testing Accuracy:%2.3f'%(acc), '% ',', Time: %3.3f'%(end_time - start_time), 's')\n","  return running_loss, acc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LqyIRLwcZ5mA"},"source":["class fetch_image_pairs(torch.utils.data.Dataset):\n","  def __init__(self,text_pair_path,dir_path,dev_pair=False):\n","   self.pair_1  = []\n","   self.pair_2  = []\n","   self.label   = []\n","   self.dev_pair = dev_pair\n","   self.dir_path = dir_path\n","\n","   with open(text_pair_path) as f:\n","     for line in f:\n","       items = line.split()\n","       self.pair_1.append(items[0])\n","       self.pair_2.append(items[1])\n","       if self.dev_pair:\n","         self.label.append(items[2])\n","       else:\n","          self.label.append(-1)\n","\n","  def __len__(self):\n","    return len(self.pair_1)\n","  \n","  def __getitem__(self,index):\n","    img1 = Image.open(self.dir_path+self.pair_1[index])\n","    img2 = Image.open(self.dir_path+self.pair_2[index])\n","    img1 = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.485,0.456,0.406),(0.229,0.224,0.225))])(img1)\n","    img2 = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.485,0.456,0.406),(0.229,0.224,0.225))])(img2)\n","    lbl = int(self.label[index])\n","    return img1,img2,lbl\n","\n","\n","def test_verify(model,vpv_loader):\n","  sim_score   = []\n","  exact_score = []\n","  tart_time = time.time()\n","  with torch.no_grad():\n","    model.eval()\n","    for batch_idx, (img1,img2,true_score) in enumerate(vpv_loader):  \n","      img1,img2,true_score = img1.to(device), img2.to(device),true_score.to(device)\n","      embedding_1 = model(img1.float())[0]\n","      embedding_2 = model(img2.float())[0]\n","      calc_score = F.cosine_similarity(embedding_1,embedding_2)\n","      sim_score.append(calc_score.view(-1))\n","      exact_score.append(true_score.view(-1))\n","      torch.cuda.empty_cache()\n","      del true_score\n","      del img1\n","      del img2\n","  end_time = time.time()\n","  print(\"Similarity score calculated:!\")\n","  return sim_score,exact_score\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RAWBUbLgFkTs"},"source":["# Hyper parameters\n","cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda\" if cuda else \"cpu\")\n","train_batch_size = 128                      # input batch size for training')\n","test_batch_size  = 64                      # input batch size for training')\n","epochs           = 10                       # number of epochs for training\n","base_lr          = 6.5e-03               # learning rate for a single GPU\n","lr_cent          = 0.5                      # learning rate for center Loss\n","weight_cent      = 1                        # Weight of the Center Loss\n","wd               = 5.0e-04                   # weight decay\n","num_workers      = 4                        # number of worksers for GPU\n","momentum         = 0.9                      # SGD momentum\n","embedding_dim    = 512                      # embedding dimension for images\n","hidden_layers    = [1,1,1,1]                # ResNET hidden Layers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xCG7dglRFd_j","executionInfo":{"status":"ok","timestamp":1603367870281,"user_tz":240,"elapsed":1709,"user":{"displayName":"shams basir","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg8NJWd8PZvQ1q4IT0m-PJSHWn2nLzMCOXym7Ydiw=s64","userId":"09640498298136079307"}},"outputId":"3a4aaa2e-ca15-45f5-af90-0e95e1abf38b","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["data_dir = \"/data/classification_data/\"; \n","dev_set = get_myDataset(data_dir,train=False,val=True,test=False)\n","kwargs=dict(shuffle=False,batch_size=test_batch_size,num_workers=num_workers,pin_memory=True,drop_last=False)\n","dev_loader = get_data_loader(dev_set,**kwargs)\n","print(len(dev_set))\n","\n","train_set = get_myDataset(data_dir,train=True,val=False,test=False)\n","kwargs=dict(shuffle=True,batch_size=train_batch_size,num_workers=num_workers,pin_memory=True,drop_last=False)\n","train_loader = get_data_loader(train_set,**kwargs)\n","print(len(train_set))\n","\n","# Now estimating the AUC for the validation pairs\n","vpv_set    = fetch_image_pairs('/data/verification_pairs_val.txt',\"/data/\",dev_pair=True)\n","vpv_loader = DataLoader(vpv_set,batch_size=test_batch_size,shuffle=False,num_workers=num_workers,drop_last=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["8000\n","380638\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"riJY7K7Yye-y","executionInfo":{"status":"ok","timestamp":1603367874059,"user_tz":240,"elapsed":360,"user":{"displayName":"shams basir","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg8NJWd8PZvQ1q4IT0m-PJSHWn2nLzMCOXym7Ydiw=s64","userId":"09640498298136079307"}},"outputId":"f6591345-4251-4668-d895-cb210c6d126c","colab":{"base_uri":"https://localhost:8080/","height":935}},"source":["# Model \n","num_classes      = len(train_set.classes)      # number of unique classes\n","#model = ResNet(hidden_layers,num_classes,embedding_dim)\n","#model.apply(init_weights)\n","# Start training from where I left off last time\n","#PATH  = os.getcwd()+f\"/saved_model/state_dict_model_2020.10.22-11:37:35_6.pt\"\n","#model.load_state_dict(torch.load(PATH))\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(),lr=base_lr,momentum = momentum,weight_decay=wd,nesterov=True)\n","#scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=3,gamma=0.85)\n","#scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=.85, last_epoch=-1, verbose=False)\n","scheduler =torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.85,\n","                                           patience=2, threshold=0.5, \n","                                           threshold_mode='rel', cooldown=0, \n","                                           min_lr=0, eps=1e-08, \n","                                           verbose=True)\n","model.to(device)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ResNet(\n","  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (layer1): Sequential(\n","    (0): baseBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (shortcut): Sequential(\n","        (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(2, 2))\n","        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): baseBlock(\n","      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (shortcut): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): baseBlock(\n","      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (shortcut): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): baseBlock(\n","      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (shortcut): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","  )\n","  (avgPool): AdaptiveAvgPool2d(output_size=2)\n","  (fc): Linear(in_features=2048, out_features=4000, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":195}]},{"cell_type":"code","metadata":{"id":"UvfB9AfAyx2x","outputId":"a6a68031-4f9f-419d-da25-4b4b7cdb3b20","colab":{"base_uri":"https://localhost:8080/","height":136}},"source":["Train_loss = []\n","Test_loss  = []\n","Test_acc   = []\n","\n","print('*-*'*20)\n","print('train_batch_size: {}, test_batch_size: {}'.format(train_batch_size,test_batch_size))\n","print('*-*'*20)\n","print('Epoch : {:2d}, Learning rate : {:.5e} ' .format(1,optimizer.param_groups[0]['lr']))\n","\n","for epoch in range(epochs):\n","  train_loss = train(model, train_loader, criterion, optimizer)\n","  test_loss, test_acc = test_model(model, dev_loader, criterion)\n","  Train_loss.append(train_loss)\n","  Test_loss.append(test_loss)\n","  Test_acc.append(test_acc)\n","  sim_score,exact_score=test_verify(model,vpv_loader)\n","  sim_score = torch.cat(sim_score,axis=0)\n","  exact_score = torch.cat(exact_score,axis=0)\n","  auc   = roc_auc_score(exact_score.cpu().numpy(),sim_score.cpu().numpy())\n","  print('='*20)\n","  print('*auc: {:.3f}'.format(auc))\n","  print('*^*'*20)\n","  save_model(model,epoch)\n","  scheduler.step(test_loss)\n","  print('Epoch : {:2d}, Learning rate : {:.5e} ' .format(epoch+1,optimizer.param_groups[0]['lr']))\n","save_output(Train_loss,'Train_loss')\n","save_output(Test_loss,'Test_loss')\n","save_output(Test_acc,'Test_acc')\n","visualize(Train_loss,'Train_loss')\n","visualize(Test_loss,'Test_loss')\n","visualize(Test_acc,'Test_acc') \n","\n","# Predicting the AUC for the test pairs\n","vpt_set    = fetch_image_pairs('/data/verification_pairs_test.txt','/data/',dev_pair=False)\n","vpt_loader = DataLoader(vpt_set,batch_size=test_batch_size,shuffle=False,num_workers=num_workers,drop_last=False)\n","sim_score,exact_score=test_verify(model,vpt_loader)\n","sim_score = torch.cat(sim_score,axis=0)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["*-**-**-**-**-**-**-**-**-**-**-**-**-**-**-**-**-**-**-**-*\n","train_batch_size: 128, test_batch_size: 64\n","*-**-**-**-**-**-**-**-**-**-**-**-**-**-**-**-**-**-**-**-*\n","Epoch :  1, Learning rate : 6.50000e-03 \n","Batch_index :   500, Avg Loss in 500 mini batches: 0.4348\n","Batch_index :  1000, Avg Loss in 500 mini batches: 0.5697\n","Batch_index :  1500, Avg Loss in 500 mini batches: 0.6503\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1YD9AhUV83sK"},"source":["# preparing the submission file "],"execution_count":null,"outputs":[]}]}